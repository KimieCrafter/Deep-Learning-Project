{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8a08b1",
   "metadata": {},
   "source": [
    "## Based on kaggle.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7baaf33",
   "metadata": {},
   "source": [
    "## Removing Useless Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0f92f",
   "metadata": {},
   "source": [
    "# Removing 2451, 47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbaca1b",
   "metadata": {},
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838be5a",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.amp.grad_scaler import GradScaler\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 16  # Smaller batch size for better memory management\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_BANDS = 100  # Keep all bands as they might be important\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT_RATE = 0.3\n",
    "NUM_FOLDS = 5\n",
    "ACCUMULATION_STEPS = 1\n",
    "PATCH_SIZE = 32  # Keep smaller patch size for faster processing\n",
    "\n",
    "class EnhancedSpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=7):\n",
    "        super(EnhancedSpatialAttention, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, kernel_size, padding=kernel_size//2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size, padding=kernel_size//2)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=1)  # Output single channel attention map\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Create attention map\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "        \n",
    "        x_att = F.relu(self.bn1(self.conv1(concat)))\n",
    "        x_att = F.relu(self.bn2(self.conv2(x_att)))\n",
    "        attention = self.sigmoid(self.conv3(x_att))\n",
    "        \n",
    "        # Apply attention to input\n",
    "        return x * attention  # Broadcasting will handle the channel dimension\n",
    "\n",
    "class EnhancedChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(EnhancedChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        self.bn = nn.BatchNorm1d(in_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        \n",
    "        avg_out = self.fc2(F.relu(self.fc1(self.avg_pool(x).view(b, c))))\n",
    "        max_out = self.fc2(F.relu(self.fc1(self.max_pool(x).view(b, c))))\n",
    "        \n",
    "        out = avg_out + max_out\n",
    "        out = self.bn(out.view(b, c))\n",
    "        return self.sigmoid(out).view(b, c, 1, 1)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.ca = EnhancedChannelAttention(out_channels)\n",
    "        self.sa = EnhancedSpatialAttention(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Apply channel attention\n",
    "        out = self.ca(out) * out\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        out = self.sa(out) * out\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class HyperspectralCNN(nn.Module):\n",
    "    def __init__(self, in_channels=NUM_BANDS):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Improved regressor with skip connections\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.regressor(x)\n",
    "        return x * 100  # Scale to 0-100 range\n",
    "\n",
    "class HyperspectralDataset(Dataset):\n",
    "    def __init__(self, df, base_path, patch_size=PATCH_SIZE, augment=False, is_test=False):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Pre-compute file paths\n",
    "        self.file_paths = [f\"{self.base_path}/{npy_file}\" for npy_file in self.df['id']]\n",
    "        \n",
    "        # Define augmentations using PyTorch transforms - simplified for speed\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([])  # Empty transform\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.df)} samples\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load data\n",
    "            data = np.load(self.file_paths[idx])\n",
    "            \n",
    "            if len(data.shape) == 3:\n",
    "                data = data.transpose(2, 0, 1)  # Change to (channels, height, width)\n",
    "            \n",
    "            if data.shape[0] != NUM_BANDS:\n",
    "                if data.shape[0] > NUM_BANDS:\n",
    "                    data = data[:NUM_BANDS]\n",
    "                else:\n",
    "                    padding = np.zeros((NUM_BANDS - data.shape[0], data.shape[1], data.shape[2]))\n",
    "                    data = np.concatenate([data, padding], axis=0)\n",
    "            \n",
    "            # Convert to torch tensor\n",
    "            data = torch.from_numpy(data).float()\n",
    "            \n",
    "            # Resize if needed - using nearest neighbor for speed\n",
    "            if data.shape[1] != self.patch_size or data.shape[2] != self.patch_size:\n",
    "                data = F.interpolate(data.unsqueeze(0), \n",
    "                                   size=(self.patch_size, self.patch_size), \n",
    "                                   mode='nearest').squeeze(0)\n",
    "            \n",
    "            # Apply augmentations if enabled and not test data\n",
    "            if self.augment and not self.is_test:\n",
    "                # Apply spatial augmentations to each channel\n",
    "                augmented_data = []\n",
    "                for i in range(data.shape[0]):\n",
    "                    channel_data = data[i:i+1]  # Keep channel dimension\n",
    "                    augmented_channel = self.transform(channel_data)\n",
    "                    augmented_data.append(augmented_channel)\n",
    "                data = torch.cat(augmented_data, dim=0)\n",
    "            \n",
    "            # Normalize data\n",
    "            data = (data - data.mean()) / (data.std() + 1e-8)\n",
    "            \n",
    "            if self.is_test:\n",
    "                return data\n",
    "            else:\n",
    "                disease_percentage = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.float32)\n",
    "                return data, disease_percentage\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, scheduler, fold):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_maes = []\n",
    "    val_maes = []\n",
    "    best_val_mae = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    scaler = GradScaler('cuda')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "        valid_samples = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            if torch.isnan(inputs).any() or torch.isnan(labels).any():\n",
    "                continue\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze(-1)  # Remove last dimension to match labels\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / ACCUMULATION_STEPS\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0) * ACCUMULATION_STEPS\n",
    "            train_mae += torch.abs(outputs - labels).sum().item()\n",
    "            valid_samples += inputs.size(0)\n",
    "        \n",
    "        if valid_samples > 0:\n",
    "            train_loss /= valid_samples\n",
    "            train_mae /= valid_samples\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_mae = evaluate_model(model, val_loader, criterion)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_maes.append(train_mae)\n",
    "            val_maes.append(val_mae)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            print(f\"Fold {fold}, Epoch {epoch+1}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.2f}%\")\n",
    "            print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_mae': val_mae,\n",
    "                }, f'best_model_fold_{fold}.pth')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return model, train_losses, val_losses, train_maes, val_maes\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device=DEVICE) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze(-1)  # Remove last dimension to match labels\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_mae += torch.abs(outputs - labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_mae = total_mae / len(loader.dataset)\n",
    "    \n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "def main():\n",
    "    print(\"Starting main function...\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv('C:/IIUM/AI Note IIUM/Deep_Learning/Project/Data/train.csv')\n",
    "    test_df = pd.read_csv('C:/IIUM/AI Note IIUM/Deep_Learning/Project/Data/test.csv')\n",
    "    base_path = 'C:/IIUM/AI Note IIUM/Deep_Learning/Project/Data/ot'\n",
    "    \n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store results for each fold\n",
    "    fold_results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n",
    "        print(f\"\\nTraining Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "        print(f\"Train samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_fold = train_df.iloc[train_idx]\n",
    "        val_fold = train_df.iloc[val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"Creating datasets...\")\n",
    "        train_dataset = HyperspectralDataset(train_fold, base_path, augment=True, is_test=False)\n",
    "        val_dataset = HyperspectralDataset(val_fold, base_path, augment=False, is_test=False)\n",
    "        \n",
    "        # Create data loaders with optimized parameters\n",
    "        print(\"Creating data loaders...\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=True, \n",
    "            num_workers=0,  # Keep at 0 for debugging\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            shuffle=False, \n",
    "            num_workers=0,  # Keep at 0 for debugging\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(\"Initializing model...\")\n",
    "        # Initialize model and training components\n",
    "        model = HyperspectralCNN().to(DEVICE)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        # Train model\n",
    "        model, train_losses, val_losses, train_maes, val_maes = train_model(\n",
    "            model, train_loader, val_loader, EPOCHS, criterion, optimizer, scheduler, fold\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_maes': train_maes,\n",
    "            'val_maes': val_maes\n",
    "        })\n",
    "        \n",
    "        # Plot training curves for this fold\n",
    "        plot_training_curves(train_losses, val_losses, train_maes, val_maes, fold)\n",
    "    \n",
    "    # Make predictions on test set using ensemble of models\n",
    "    test_dataset = HyperspectralDataset(test_df, base_path, augment=False, is_test=True)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    all_preds = []\n",
    "    for fold in range(NUM_FOLDS):\n",
    "        # Load best model for this fold\n",
    "        checkpoint = torch.load(f'best_model_fold_{fold}.pth')\n",
    "        model = HyperspectralCNN().to(DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs in tqdm(test_loader, desc=f\"Making predictions (Fold {fold+1})\"):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                fold_preds.extend(outputs.cpu().numpy())\n",
    "        \n",
    "        all_preds.append(fold_preds)\n",
    "    \n",
    "    # Average predictions from all folds\n",
    "    final_preds = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'predicted_percentage': final_preds\n",
    "    })\n",
    "    results_df.to_csv('test_predictions.csv', index=False)\n",
    "    \n",
    "    # Plot ensemble results\n",
    "    plot_ensemble_results(fold_results)\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_maes, val_maes, fold):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='o')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'Training and Validation Loss (Fold {fold+1})')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(train_maes, label='Train MAE', marker='o')\n",
    "    ax2.plot(val_maes, label='Val MAE', marker='o')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAE (%)')\n",
    "    ax2.set_title(f'Training and Validation MAE (Fold {fold+1})')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_curves_fold_{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_ensemble_results(fold_results):\n",
    "    # Plot average performance across folds\n",
    "    avg_train_losses = np.mean([r['train_losses'] for r in fold_results], axis=0)\n",
    "    avg_val_losses = np.mean([r['val_losses'] for r in fold_results], axis=0)\n",
    "    avg_train_maes = np.mean([r['train_maes'] for r in fold_results], axis=0)\n",
    "    avg_val_maes = np.mean([r['val_maes'] for r in fold_results], axis=0)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(avg_train_losses, label='Avg Train Loss', marker='o')\n",
    "    ax1.plot(avg_val_losses, label='Avg Val Loss', marker='o')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Average Training and Validation Loss Across Folds')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(avg_train_maes, label='Avg Train MAE', marker='o')\n",
    "    ax2.plot(avg_val_maes, label='Avg Val MAE', marker='o')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('MAE (%)')\n",
    "    ax2.set_title('Average Training and Validation MAE Across Folds')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ensemble_results.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
